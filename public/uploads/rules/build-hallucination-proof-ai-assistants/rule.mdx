---
authors:
- title: Calum Simpson
  url: https://www.ssw.com.au/people/calum-simpson/
created: 2025-06-24 05:43:09+00:00
createdBy: Louis Roa [SSW]
createdByEmail: 149936952+louisroa8189@users.noreply.github.com
guid: da2bf3fa-4ef4-4361-8513-2ed48f63c189
isArchived: false
lastUpdated: 2025-10-09 06:49:21+00:00
lastUpdatedBy: Tiago AraÃºjo [SSW]
lastUpdatedByEmail: tiagov8@gmail.com
related:
- rule: public/uploads/rules/avoid-ai-hallucinations/rule.mdx
seoDescription: Learn how to layer retrieval, guardrails, and prompt-sanitization
  so your AI assistant never â€œmakes it upâ€ in finance, healthcare, legal, or other
  high-stakes domains.
tips: ''
title: Do you build hallucination-proof AI assistants?
type: rule
uri: build-hallucination-proof-ai-assistants
---

<introEmbed
  body={<>
â€œYour loan is approved under Section 42 of the Banking Act 2025.â€
One problem: **there is no Section 42**.

That single hallucination triggered a regulator investigation and a six-figure penalty. In high-stakes domains like finance, healthcare, legal and compliance **zero-error tolerance** is the rule. Your assistant must _always_ ground its answers in real, verifiable evidence.
  </>}
/>
## 1 â€“ Why high-stakes domains punish guesswork

* Regulatory fines, licence suspensions, lawsuits
* Patient harm or misdiagnosis
* Massive reputational damage and loss of trust

When the error budget is effectively **0%**, traditional â€œchat styleâ€ LLMs are not enough.

## 2 â€“ The three-layer defense against hallucination

### 2.1  Retrieval-Augmented Generation (RAG)

* **What it does** â€“ Pulls fresh text from authoritative sources (regulations, peer-reviewed papers, SOPs) **before** answering.
* **Win** â€“ Grounds every claim in evidence; supports â€œlatest versionâ€ answers.
* **Risk** â€“ Garbage in, garbage out. A bad retriever seeds bad context.

### 2.2  Guardrail filter

* **What it does** â€“ Post-processes the draft answer. Blocks responses that:
  * lack citations
  * creep into forbidden advice (medical, legal)
  * include blanket â€œalways/neverâ€ claims
* **Win** â€“ Catches risky output before it reaches the user.
* **Risk** â€“ Over-filtering if rules are too broad or vague.

### 2.3  Question sanitizer

* **What it does** â€“ Rewrites the user prompt, removing ambiguity and hidden assumptions so retrieval hits the right documents.
* **Win** â€“ Sharper queries â‡’ cleaner answers.
* **Risk** â€“ Requires strong NLU to keep the chat natural.


<asideEmbed
  variant="greybox"
  body={<>
    Raw prompt
&gt; â€œIs this drug safe for kids?â€

Sanitized prompt
&gt; â€œAccording to current Therapeutic Goods Administration (Australia) guidelines, what is the approved dosage and contraindication list for **Drug X** in children aged **6â€“12 years**?â€
  </>}
  figureEmbed={{
    preset: "goodExample",
    figure: "Good example â€“ Sanitization adds age range, official source, and specific drug name",
    shouldDisplay: true
  }}
/>


> **Rule of thumb:** _Use **all three** layers. One patch isnâ€™t enough._

## 3 â€“ Reference architecture

1. **Vector store & embeddings** â€“ Pick models that benchmark well on MTEB; keep the DB pluggable (FAISS, Pinecone, Azure Cognitive Search).
2. **Retriever tuning** â€“ Measure recall@k, MRR, NDCG; test different chunk sizes and hybrid search.
3. **Foundation model & versioning** â€“ Record the model hash in every call; monitor LiveBench for regressions.
4. **Guardrails** â€“ Combine rule-based (regex) and model-based tools (OpenAI Guardrails, Nvidia Nemotron Guardrails).
5. **Audit logging** â€“ Append-only logs of user prompt, retrieval IDs, model version, guardrail outcome.

## 4 â€“ Measurement is mandatory ğŸ§ª

Track from Day 0:

* **Exact-answer accuracy** (human-graded)
* **Citation coverage** (every claim cited)
* **Compliance errors** (dosage mismatch, policy breach)
* **Hallucination rate** (uncited claims)
* **Retrieval miss rate** (index drift, ACL failures)

## 5 â€“ Scaling safely

| Stage | Accuracy target | Traffic share | Human-in-loop |
|-------|-----------------|--------------|---------------|
| Shadow mode | â‰¥ 80 % observed | 0 % | 100 % offline review |
| Pilot / augment | â‰¥ 80 % | ~5 % | Mandatory review |
| Limited release | â‰¥ 95 % on top queries | ~25 % | Spot check |
| Full automation | â‰¥ 99 % + zero critical | 100 % | Exception only |

Auto-fallback to a human expert if any metric dips below threshold.

## 6 â€“ Domain experts are non-negotiable

* **Source curation** â€“ SMEs tag â€œgoldâ€ paragraphs; retriever ignores the rest.
* **Prompt reviews** â€“ Experts catch edge cases outsiders miss.
* **Error triage** â€“ Every failure labeled with **why** it failed (retrieval miss, guardrail gap, model hallucination).

Treat specialists as _co-developers_, not QA afterthoughts.

## 7 â€“ Key takeaways

* **Layer it on** â€“ RAG + sanitization + guardrails deliver the most robust defense.
* **Measure everything** â€“ Strict, automated metrics keep you honest.
* **Log & secure by default** â€“ ACLs, encryption, append-only audit trails.
* **Scale with care** â€“ Stay human-in-the-loop until the data proves otherwise.

Nail these practices and youâ€™ll move from a flashy demo to a production-grade AI assistant that never makes up the rules or facts.