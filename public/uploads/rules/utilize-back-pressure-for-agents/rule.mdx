---
type: rule
title: Do you use back pressure to keep AI agents on track?
uri: utilize-back-pressure-for-agents
categories:
  - category: categories/artificial-intelligence/rules-to-better-ai.mdx
authors:
  - title: Eddie Kranz
    url: 'https://www.ssw.com.au/people/eddie-kranz'
related:
  - rule: public/uploads/rules/ai-cli-tools/rule.mdx
  - rule: public/uploads/rules/use-mcp-to-standardize-llm-connections/rule.mdx
  - rule: public/uploads/rules/critical-agent/rule.mdx
  - rule: public/uploads/rules/mcp-servers-for-context/rule.mdx
guid: f63f04e9-36c1-4e06-b21c-217bae75811f
seoDescription: 'Learn how to use back pressure (compilers, linters, tests, MCP servers) to improve AI agent reliability and reduce manual review time.'
created: 2026-01-29T00:00:00.000Z
createdBy: Eddie Kranz
createdByEmail: EddieKranz@ssw.com.au
lastUpdated: 2026-01-29T05:36:42.463Z
lastUpdatedBy: Eddie Kranz
lastUpdatedByEmail: EddieKranz@ssw.com.au
---

When working with [AI agents](/ai-cli-tools), a common bottleneck is the human review loop. If you ask an agent to "refactor this class" and it introduces a syntax error, you have to spot it and tell the agent to fix it. This manual feedback loop is slow, unscalable, and wastes your time.

<endIntro />

"Back pressure" refers to the automated feedback provided by your environment (compilers, [linters](/do-you-use-the-code-health-extensions-in-vs-code), [test runners](/the-different-types-of-test), [type systems](/good-typescript-configuration)) that constrains the agent's output. By exposing these signals to the agent, you allow it to self-correct and learn from its mistakes without your intervention, allowing agents to run for longer, with higher quality final output.

## Why use it?

Without back pressure, an AI agent relies entirely on you to validate its work. You become the validator. This limits the complexity of tasks you can delegate because the cost of verifying the work increases linearly with the task size.

<boxEmbed
  style="greybox"
  body={<>
    User: "Refactor the UserProfile component to use the new API."
    
    Agent: Generates code "Here you go."
    
    User: Pastes into IDE ‚Üí ‚ùå Error: 'UserAvatar' is not defined.
    
    User: "You missed the import for UserAvatar."
    
    Agent: "Apologies." Fixes it
  </>}
  figurePrefix="bad"
  figure="Bad example - The human wastes time acting as the validator"
/>

With back pressure, the agent operates in a closed loop:

1. Agent generates code
2. Agent runs validation tools (Build/Test/Lint)
3. System returns errors (Back Pressure)
4. Agent analyzes errors and iterates
5. Success ‚Üí Only then does it request human review

## Examples

### 1. Compilation and Linting

The most immediate form of back pressure comes from the build system.

This is already built into most coding agents.

<boxEmbed
  style="greybox"
  body={<>
    User: "Refactor the UserProfile component to use the new API."
    
    Agent: Generates code
    
    Agent: Runs npm run lint ‚Üí ‚ùå Error: 'UserAvatar' is not defined.
    
    Agent: Reads error ‚Üí "I need to import UserAvatar."
    
    Agent: Fixes code ‚Üí Runs lint ‚Üí ‚úÖ Success.
    
    Agent: "I have refactored the component and verified it passes linting."
  </>}
  figurePrefix="good"
  figure="Good example - The agent uses the linter to self-correct before bothering the user"
/>

### 2. Automated Tests

You can ask agents to write and run [tests](/the-different-types-of-test) to verify logic, not just syntax.

<boxEmbed
  style="greybox"
  body={<>
    User: "Write a function to validate email addresses."
    
    Agent: Writes regex "Here it is."
    
    User: "This accepts 'user\@domain' without a TLD."
    
    Agent: "Let me fix that."
  </>}
  figurePrefix="bad"
  figure="Bad example - Logic errors are caught manually by the human"
/>

<boxEmbed
  style="greybox"
  body={<>
    User: "Write a function to validate email addresses. Create a test suite to verify it."
    
    Agent: Writes function & tests
    
    Agent: Runs tests ‚Üí ‚ùå Failure: 'user\@domain' should be invalid.
    
    Agent: Adjusts regex ‚Üí Runs tests ‚Üí ‚úÖ Pass.
    
    Agent: "Function created and verified against 10 test cases."
  </>}
  figurePrefix="good"
  figure="Good example - The agent uses tests as back pressure to ensure correctness"
/>

<boxEmbed
  style="info"
  body={<>
    Tip: When asking an agent to write its own tests, be careful it doesn't write "lazy" tests that always pass. You might need to review the tests themselves or ask it to generate tests before the implementation.
  </>}
  figurePrefix="none"
  figure="Tip: Verify the verifier"
/>

### 3. Type Systems

Strongly typed languages ([TypeScript](/good-typescript-configuration), C#, Rust) provide excellent back pressure. The type checker creates a "guard rail" that prevents the agent from making invalid assumptions about data structures.

This is already built into coding agents, so you don't need to worry about it.

### 4. MCP Servers & Runtime Tools

Advanced agents can use [Model Context Protocol (MCP) servers](/mcp-server) to interact with real runtime environments. This allows them to "spin things up" and verify they actually work.

[Playwright](/microsoft-recommended-frameworks-for-automated-ui-driven-functional-testing): The agent can write a web app, start a local server, and run Playwright tests against it to verify the UI works as expected, it can even take screenshots of the UI and compare them to the expected state.

.NET Aspire MCP: The agent can use Aspire to orchestrate a distributed application and verify service-to-service communication, by reading console logs etc.

### 5. Proof Assistants (Sub-agents)

You can structure your workflow so that one agent generates code and a second ["Reviewer"](/automatic-code-reviews-github) agent critiques it. The Reviewer acts as a proof assistant, rejecting code that doesn't meet specific guidelines or security standards, providing back pressure before the human ever sees it.

### 6. The Human (Final Back Pressure)

If all else fails, you are the final layer of back pressure. ü´µ

You should only be rejecting work based on high-level architecture, business logic, or user experience issues.

If you are rejecting work because of syntax errors or failing tests, you are "wasting" your back pressure on things machines can solve.

***

The goal is to stop treating agents as "text generators" and start treating them as "developers" that must prove their work compiles and runs before you see it.

### Further Reading:

* [https://banay.me/dont-waste-your-backpressure](https://banay.me/dont-waste-your-backpressure)
* [https://ghuntley.com/pressure/](https://ghuntley.com/pressure/)
