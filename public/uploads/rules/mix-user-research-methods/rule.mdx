---
authors:
- title: Betty Bondoc
  url: https://www.ssw.com.au/people/betty-bondoc/
categories:
- category: categories/design/rules-to-better-designers.mdx
- category: categories/software-engineering/rules-to-better-testing.mdx
created: 2025-07-21 03:45:25+00:00
createdBy: Betty Bondoc [SSW]
createdByEmail: 33026270+bettybondoc@users.noreply.github.com
guid: 8fc0259e-f4b9-48bd-9c6c-565677b7b841
isArchived: false
lastUpdated: 2025-09-15 12:18:01+00:00
lastUpdatedBy: Gilles Pothieu [SSW]
lastUpdatedByEmail: gillespothieu@ssw.fr
related:
- rule: public/uploads/rules/test-high-risk-features/rule.mdx
- rule: public/uploads/rules/measure-website-changes-impact/rule.mdx
seoDescription: Don’t rely only on 1-on-1 interviews. Mix research methods to uncover
  both personal pain points and shared behaviours across teams.
tips: ''
title: Do you mix user research methods to capture the full picture?
type: rule
uri: mix-user-research-methods
---

Relying on the same user research method like only running 1-on-1 interviews leads to blind spots. Interviews are great for depth, but they don’t reveal group dynamics, cross-team dependencies, or patterns that emerge at scale.

To design experiences that scale, you need insights that are **qualitative** and **quantitative**, **attitudinal** and **behavioural**, and drawn from both **individuals** and **teams**.

<endIntro />

## Different research methods

### Attitudinal vs behavioral

**Attitudinal** research captures what users **say** — their opinions, preferences, and perceptions. E.g. User surveys saying a feature is confusing.

**Behavioral** research focuses on what users **actually do** during real interactions. E.g. Usability testing shows users skip the feature entirely.

### Qualitative vs quantitative

**Qualitative** research helps you understand **why** something is happening by exploring deeper motivations and thought processes. E.g. Open-ended interviews reveal frustration with a checkout step.

**Quantitative** research measures **how often** something occurs through numbers and patterns. E.g. Analytics show a 60% drop-off rate on the same step.

### Context of use

The **way users are studied** can vary by how closely it reflects real-world use. E.g. A team conducts in-person usability testing with real customers performing typical tasks in a production app (natural + scripted context).

* **Natural use** observes users in their actual environment. E.g. Field study
* **Scripted tasks** have users perform specific actions. E.g. Usability tests
* **Abstracted activities** use remote or synthetic scenarios E.g. card sorting, A/B tests

## Why mix research methods?

According to Nielsen Norman Group, no single method uncovers all UX insights. A mix of approaches leads to better understanding and fewer blind spots.

✅ Mixing research methods helps you:

* Understand both individual workflows and systemic issues
* Avoid designing for edge cases or only the loudest voices
* Uncover hidden workarounds and friction between teams
* Generate more reliable, well-rounded insights

Read the full article: [When to Use Which User-Experience Research Methods](https://www.nngroup.com/articles/which-ux-research-methods/).


<youtubeEmbed url="https://www.youtube.com/watch?v=OtUWbsvCujM" description={"Video: When to Use Which UX Research Method (5 min)"} />


## What methods should I combine?

| Method                         | Use it to                                               |
|-------------------------------|-------------------------------------------------------------|
| **1-on-1 interviews**             | Explore personal workflows and uncover pain points         |
| **Group workshops / focus groups** | Reveal shared behaviors and team-level challenges          |
| **Observation / shadowing**       | Catch things users don’t say (but still do)                |
| **Surveys**                       | Quantify trends or confirm emerging patterns across users  |

You don’t need to use every method on every project, but most projects benefit from using at least two.

## Practical example


<boxEmbed
  style="greybox"
  body={<>
    A UX designer begins by interviewing three Admins to understand how they use the system. Based on those findings, she runs a 60-minute group workshop with Admins and Technicians to map shared issues and uncover workflow gaps between teams.
  </>}
  figurePrefix="good"
  figure={"Good example – Combines depth from interviews with breadth from group insight"}
/>



<boxEmbed
  style="greybox"
  body={<>
    Only interviewing one user and assuming their experience applies to everyone. Skipping group validation and relying on stakeholder assumptions.
  </>}
  figurePrefix="bad"
  figure={"Bad example – Designs based on limited or biased input"}
/>


## Bonus tip: Mix user roles, not just methods

Group sessions are a great way to bring in users from different teams, departments, or levels of experience. Doing so can help you uncover:

* Gaps in communication
* Conflicting priorities
* “Unofficial” workarounds users rely on

These types of issues rarely come up in 1-on-1 interviews alone.