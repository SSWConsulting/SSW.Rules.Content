---
title: Do you follow best practices for prompt caching?
uri: do-you-follow-best-practices-for-prompt-caching
categories:
  - category: categories/artificial-intelligence/rules-to-better-ai-development.mdx
authors:
  - title: Michael Smedley
    url: 'https://www.ssw.com.au/people/michael-smedley/'
related:
  - {}
guid: 59c83e62-5c27-4bd4-9d90-6f774983acac
seoDescription: 'Learn best practices for prompt caching in OpenAI-style workflows, including deterministic prefixes, cache key design, tool stability, and instrumentation for maximum performance and cost efficiency.'
created: 2026-02-19T23:48:44.539Z
createdBy: Mike
createdByEmail: michaelsmedley@ssw.com.au
lastUpdated: 2026-02-20T03:57:27.350Z
lastUpdatedBy: Mike
lastUpdatedByEmail: michaelsmedley@ssw.com.au
---

You ship a new AI feature and expect costs and latency to drop thanks to prompt caching — but nothing changes. After investigation, you discover that a timestamp in the system prompt or a small tool schema change invalidated the cache on every request. Small structural mistakes can completely eliminate caching benefits.

<endIntro />

Prompt caching (such as OpenAI-style implicit KV caching) can dramatically reduce latency and cost — but only if your prompts are structured deliberately. Follow these rules to maximize cache hit rates and keep your architecture efficient.

## Treat the prompt like a deterministic artifact

Caching requires an identical, contiguous prefix. Even minor differences break reuse.

<boxEmbed
  body={<>
    * Keep your system prompt static
    * Keep tool definitions and schemas stable
    * Maintain identical ordering of messages
    * Ensure consistent whitespace and formatting
  </>}
  figurePrefix="good"
  figure=""
  style="greybox"
/>

<boxEmbed
  body={<>
    ❌ Don’t
    
    * Inject timestamps or random IDs near the beginning
    * Reorder tools or messages
    * Introduce small per-request formatting differences
  </>}
  figurePrefix="none"
  figure=""
  style="greybox"
/>
