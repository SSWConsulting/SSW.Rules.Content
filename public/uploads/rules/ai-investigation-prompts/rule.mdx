---
title: Do you use AI to explore unfamiliar code?
uri: ai-investigation-prompts
categories:
  - category: categories/artificial-intelligence/rules-to-better-ai.mdx
authors:
  - title: Gordon Beeming
    url: 'https://ssw.com.au/people/gordon-beeming'
  - title: Dan Mackay
    url: 'https://www.ssw.com.au/people/daniel-mackay/'
  - title: Luke Cook
    url: 'https://ssw.com.au/people/luke-cook'
guid: 75a805bb-35b0-419e-94e3-5bd0ba77d86d
created: 2026-01-27T23:22:58.602Z
createdBy: Luke Cook
createdByEmail: LukeCook@ssw.com.au
lastUpdated: 2026-01-28T03:24:13.115Z
lastUpdatedBy: Luke Cook
lastUpdatedByEmail: LukeCook@ssw.com.au
---

Spinning up on an unfamiliar project used to be a stressful experience - it could take developers months to learn the architecture, code patterns, and libraries that are used. And that's not even talking about the application's features and functionality.

But with intelligent use of AI, this on-boarding time can be reduced to almost zero. We can now get on-demand analysis into projects, bugs, and features in minutes, and be operating at maximum velocity in Sprint 1. Learn how to do it right.

<endIntro />

## The old way - RTFM

Historically, when you were first put onto a new project, you'd immediately look for documentation. Architecture diagrams, specifications, class diagrams, etc. If you were lucky enough to find them, you'd pray they were up-to-date with the actual code. And even in those best-case scenarios, you'd still have to translate that information into finding the code you were meant to be working on.

All of this took time, and would often send you down programming cul-de-sacs when you discovered features, requirements, or behaviour differed from the literature. The only way to know what was actually going on was to start digging through the source code.

## The new way - Investigation Prompts

Whenever SSW start on a new project, we ask AI to give us an analysis of whatever feature or bug we're looking at. Instead of spending hours (or days) exploring an unfamiliar codebase, you can get on-demand documentation of exactly what, where, and how the feature is implemented.

But producing a useful report without [AI slop](https://www.ssw.com.au/rules/avoid-automation-bias) takes some skill.

<boxEmbed
  body={<>
    I am working on Xero integration. Tell me how it's implemented.
  </>}
  figurePrefix="bad"
  figure="Vague prompt with no specific outcome or information"
  style="greybox"
/>

Giving AI broad questions leaves the door open for the model to make any number of assumptions on what you're trying to learn. The results are of little value. Instead, be specific about what you want the model to tell you.

### Provide specific objectives

Define your “dream documentation” for the specific area you are working on, and be explicit with what information you want AI to produce for you. This gives more meaningful insights, and will show you exactly which areas of the codebase you should be focusing on.

<boxEmbed
  body={<>
    I am working on the Xero integration. Give me a an architectural overview of how Xero integration is currently implemented, along with all configuration values that are used (and where to find them). Additionally, highlight the code paths that touch the Xero integrations.
  </>}
  figurePrefix="ok"
  figure="Specific objectives and outcomes you want to know"
  style="greybox"
/>

This is a great start, but you can get even better results with a bit more guidance.

### Ask clarifying questions

While the above prompt is great for pointing AI at your desired outcome, you don't know what other context the model would benefit from. Instead of trying to guess anything and everything up-front, instruct the model to come back to you with the questions it wants answered.

<boxEmbed
  body={<>
    I am working on the Xero integration. Give me a an architectural overview of how Xero integration is currently implemented, along with all configuration values that are used (and where to find them). Additionally, highlight the code paths that touch the Xero integrations.
    
    Ask me if you have any questions before you start. Ask each question one at a time. Wait for an answer before asking the next question. If there are several options, show these in a table with options labeled A, B, C, etc.
  </>}
  figurePrefix="good"
  figure="Specific objectives, and behavioural instructions"
  style="greybox"
/>

This small addition has huge pays-offs. Allowing the model to clarify your intent by asking questions you may not have thought of will drastically improve the outcome.

These 2 strategies combined will produce exceptional results, and are used by SSW daily. 

But if you're looking for something specific, such as a bug, you can get even better results with an extra instruction…

### Look through previous commits

How many times have you heard something like “This bug only started happening a week ago. Everything was working fine until then”?

So you start diffing files, and poring over earlier commits to see what changed. This is another area where AI can 10x your efficiency when investigating a problem.

<boxEmbed
  body={<>
    I am working on the Xero integration. Give me a an architectural overview of how Xero integration is currently implemented, along with all configuration values that are used (and where to find them). Additionally, highlight the code paths that touch the Xero integrations.
    
    Ask me if you have any questions before you start. Ask each question one at a time. Wait for an answer before asking the next question. If there are several options, show these in a table with options labeled A, B, C, etc.
    
    Specifically, I am investigating a bug that was introduced within the last 2 weeks. Examine the commit history and highlight changes in these code areas that should be examined further.
  </>}
  figurePrefix="good"
  figure="AI can analyse diffs much faster than you"
  style="greybox"
/>

### Create a living document

Most of the time, your investigative session isn't just a “one and done” prompt. You'll examine the output, make further inquiries, and narrow/broaden the scope based on what you learn. For these reasons, you should instruct your model to output its findings to a source-controlled document that it can read and write throughout the session.

<boxEmbed
  body={<>
    I am working on the Xero integration. Give me a an architectural overview of how Xero integration is currently implemented, along with all configuration values that are used (and where to find them). Additionally, highlight the code paths that touch the Xero integrations.
    
    Ask me if you have any questions before you start. Ask each question one at a time. Wait for an answer before asking the next question. If there are several options, show these in a table with options labeled A, B, C, etc.
    
    Output your findings to /investigations/xero-integration.md If there are changes to the scope or findings, update the document accordingly. Give a high-level summary at the beginning, with detailed breakdowns of each finding below.
  </>}
  figurePrefix="good"
  figure="Creating a source-controlled, living document for iterations"
/>

## Further reading

If you're interested in learning how SSW's Investigation Prompts came about, and their effectiveness in real-world use cases, check out the following blog posts by a couple of SSW's Solution Architects:

* Gordon Beeming: [My "Meta-Prompt" for Codebase Investigations](https://gordonbeeming.com/blog/2025-11-06/my-meta-prompt-for-codebase-investigations-with-copilot-cli-and-copilot-here)
* Dan Mackay: [Efficient Vibe Coding: How clarifying questions make AI actually useful](https://www.dandoescode.com/blog/efficient-vibe-coding-with-clarifying-questions)
