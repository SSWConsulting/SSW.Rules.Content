---
type: rule
tips: ""
title: Do you handle AI Hallucinations the right way?
seoDescription: AI hallucinations are inevitable, but with the right techniques,
  you can minimize their occurrence and impact.
uri: avoid-ai-hallucinations
authors:
  - title: Eddie Kranz
    url: https://www.ssw.com.au/people/eddie-kranz/
  - title: Calum Simpson
    url: https://www.ssw.com.au/people/calum-simpson/
related:
  - rules-to-better-chatgpt-prompt-engineering
created: 2025-03-19T18:41:21.000Z
guid: e4e963e4-1568-4e47-b184-d2e96bc0f124
lastUpdated: 2025-04-30T22:44:27.000Z
lastUpdatedBy: Calum Simpson [SSW]
lastUpdatedByEmail: calumjs@live.com
createdBy: Eddie Kranz [SSW]
createdByEmail: ed@kranz.au
---
<introEmbed
  body={<>
AI is a powerful tool, however, sometimes it simply makes things up, aka hallucinates. AI hallucinations can sometimes be humorous, but it is very bad for business!

AI hallucinations are inevitable, but with the right techniques, you can minimize their occurrence and impact. Learn how SSW tackles this challenge using proven methods like clean data tagging, multi-step prompting, and validation workflows.
  </>}
/>
**Let's face it. AI will always hallucinate.**

AI models like GPT-4o are powerful but imperfect. They generate plausible-sounding but incorrect or nonsensical outputs (hallucinations) due to training limitations, ambiguous prompts, or flawed data retrieval. While you can’t eliminate hallucinations entirely, you can **reduce their frequency** and **mitigate risks**.

---

## Use Clean, Tagged Data for RAG

```python
documents = ["Sales grew 10% in 2023", "Server downtime: 5hrs in Q2"]
```

<asideEmbed
  variant="greybox"
  body={<>
    **Query:** "What was the server uptime in Q2?"
**Hallucination:** "Server uptime was 95%." (Wrong: No uptime data exists!)
  </>}
  figureEmbed={{
    preset: "badExample",
    figure: 'Bad example - Untagged, messy data leads to garbage outputs',
    shouldDisplay: true
  }}
/>

```python
documents = [
  {"text": "Sales grew 10% in 2023", "tags": ["finance", "sales"]},
  {"text": "Server downtime: 5hrs in Q2", "tags": ["IT", "downtime"]}
]
```

<asideEmbed
  variant="greybox"
  body={<>
    **Query:** "What was the server uptime in Q2?"
**Output:** "No uptime data found. Available data: 5hrs downtime." ✅
  </>}
  figureEmbed={{
    preset: "goodExample",
    figure: 'Good example - Properly tagged data reduces the risk of incorrect retrieval',
    shouldDisplay: true
  }}
/>

## Break Workflows into Multi-Step Prompts

Use a **chain-of-thought** approach to split tasks into smaller, validated steps

<asideEmbed
  variant="greybox"
  body={<>
    **User:** "Write a blog about quantum computing benefits for SMEs."
**AI:** (Hallucinates fictional case studies and stats)
  </>}
  figureEmbed={{
    preset: "badExample",
    figure: 'Bad example - A single-step prompt invites hallucinations',
    shouldDisplay: true
  }}
/>

<asideEmbed
  variant="greybox"
  body={<>
    **User:** "Generate a blog draft about quantum computing for SMEs."\
"Verify all claims in this draft against trusted sources."\
"Compare the final draft to the original query. Did you answer the question?"
  </>}
  figureEmbed={{
    preset: "goodExample",
    figure: 'Good example - Multi-step validation reduces errors',
    shouldDisplay: true
  }}
/>

## Force the AI to Justify Its Reasoning

Always prompt the AI to **cite sources** and **flag uncertainty**.

<asideEmbed
  variant="greybox"
  body={<>
    **User:** "Why should SMEs adopt quantum computing?"
**AI:** "It boosts efficiency by 200%." (Source? None!)
  </>}
  figureEmbed={{
    preset: "badExample",
    figure: 'Bad example - No justification = unchecked errors',
    shouldDisplay: true
  }}
/>

<asideEmbed
  variant="greybox"
  body={<>
    **System Prompt:** "Answer the question and cite sources. If uncertain, say 'I don’t know'."
**User:** "Why should SMEs adopt quantum computing?"
**AI:** "Quantum computing can optimize logistics (Source: IBM, 2023). However, adoption costs may be prohibitive for SMEs."
  </>}
  figureEmbed={{
    preset: "goodExample",
    figure: 'Good example - Require citations and self-reflection',
    shouldDisplay: true
  }}
/>

## Validate Outputs Against the Original Question

Use a **validation layer** to ensure outputs align with the original query.

<asideEmbed
  variant="greybox"
  body={<>
    **User:** "How does Azure Kubernetes Service (AKS) simplify deployment?"
**AI:** Explains Kubernetes basics (ignores AKS specifics).
  </>}
  figureEmbed={{
    preset: "badExample",
    figure: 'Bad example - No final check = off-topic answers',
    shouldDisplay: true
  }}
/>

<asideEmbed
  variant="greybox"
  body={<>
    System Prompt: "Compare your answer to the user’s question. Did you address AKS?"
AI: "Revised answer: AKS simplifies deployment by integrating with Azure DevOps and..." ✅
  </>}
  figureEmbed={{
    preset: "goodExample",
    figure: 'Good example - Add a final validation step',
    shouldDisplay: true
  }}
/>

## Other techniques to minimize hallucinations

* **Lower temperature settings**: Reduce creativity (e.g., `temperature=0.3`) for factual tasks
* **Human-in-the-loop**: Flag low-confidence responses for manual review
* **Predefined constraints**: Example: "Do not speculate beyond the provided data"

---

AI hallucinations are unavoidable, but SSW’s proven techniques, like clean data tagging, multi-step validation, and forcing justification can keep them in check. By designing workflows that **anticipate errors** and **validate outputs**, you turn a risky limitation into a manageable challenge.

Always assume hallucinations **will** happen, so build systems to catch them!
