---
authors:
- title: Eddie Kranz
  url: https://www.ssw.com.au/people/eddie-kranz
categories:
- category: categories/artificial-intelligence/rules-to-better-ai.mdx
created: 2025-10-17 05:12:37+00:00
createdBy: Eddie Kranz [SSW]
createdByEmail: ed@kranz.au
guid: 67B27A09-5546-4AD9-9DCC-732CE63D7183
isArchived: false
lastUpdated: 2025-10-23 17:26:32+00:00
lastUpdatedBy: Eddie Kranz [SSW]
lastUpdatedByEmail: ed@kranz.au
related:
- rule: public/uploads/rules/manage-security-risks-when-adopting-ai-solutions/rule.mdx
- rule: public/uploads/rules/manage-legal-implications-of-ai/rule.mdx
- rule: public/uploads/rules/mitigate-brand-risks-ai/rule.mdx
- rule: public/uploads/rules/use-ai-responsibly/rule.mdx
- rule: public/uploads/rules/checked-by-xxx/rule.mdx
seoDescription: Did you know that AI has the potential to make your work worse? Learn
  the common pitfalls so that AI can help, not hinder.
tips: ''
title: Do you avoid Automation Bias? (aka AI Slop)
type: rule
uri: avoid-automation-bias
---

AI has been championed as a huge productivity booster, which it is. However, people blindly trusting AI outputs, or preferring them over theirs, can lead to 'automation bias'.

Let's talk about what leads to creating AI slop:

<endIntro />

<youtubEmbed url="https://www.youtube.com/embed/kDS5pwelhNM?si=_nqw5XQWgpib5b8z" description="Video: How AI is making you dumber (3 min)" />

Automation bias is a type of cognitive bias. A thinking shortcut, where we overly rely on machine-generated decisions and forego critical thinking. Itâ€™s a brain problem, not a model problem.

## Errors from automation bias

There are two types of errors made due to automation bias:

* **Error of commission** - Doing the wrong thing because the system suggested it (e.g. accepting a flawed suggestion, getting ChatGPT to write your essay, etc.)
* **Error of omission** - Failing to act because the system didn't flag a problem (e.g. shipping flawed code, because Copilot review approved it)

## Where automation bias shows up

* **Coding assistants (e.g. Copilot)** - They make you faster, not better. The more complex some generated code is, the more likely you'll be to click "Accept"
* **Google Search (especially 'AI Overview')** - These search engines are incentivised to return the most relevant content, rather than the most correct content
* **GPS Navigation** - Countless cases of people driving their car into water 'because the app told them to' (it's happened dozens of times!)

## Why do we fall for it?

### Task complexity + time pressure

Under stress, cognition narrows and we rely more heavily on mental shortcuts.

AI tools initially promise to "do less work," but this productivity gain often shifts expectations, leading to "do more work in the same time".

This creates a vicious cycle: Increased time pressure â†’ more likely to accept AI outputs without scrutiny.

### Social aspects & accountability

If there are two people within a company who don't know each other well, and review each other's code, they are less likely to care about what each other thinks, and they'll put less effort into a review.

### Personality traits

More extroverted people want to move quickly â†’ they accept AI outputs more readily.

People high in neuroticism may doubt their own outputs â†’ they doubt AI's outputs too.

This isn't prescriptive, just a pattern to be aware of.

## What doesn't fix it (but seems like it should)

* **Human-in-the-loop** - This only covers legal accountability. Just because a human is in the loop, doesn't mean they're immune to automation bias
* **Higher model accuracy** - Using a highly accurate model breeds complacency
* **Training people that "software is often wrong"** - Makes people think they know more about the software than they do â†’ false confidence

## What actually helps

### Don't submit AI content that you don't fully understand

If AI helps you create work, that's great.

But what if someone asks you a technical question about the work, that you can't answer?

It then becomes very obvious that this work is not your own - worse, it might even mean the solution is suboptimal.

Remember to always **own your work**.

### Show error rates clearly (for devs)

When creating tools that are prone to mistakes, include exact error rates:


<boxEmbed
  style="greybox"
  body={<>
    AI sometimes makes mistakes, ensure you check its output.
  </>}
  figurePrefix="bad"
  figure="Bad example - Highly vague warning, most people will ignore it"
/>



<boxEmbed
  style="greybox"
  body={<>
    Studies have found that 25% of AI-generated code has security vulnerabilities. Ensure you review it.
  </>}
  figurePrefix="good"
  figure="Good example - More specific warning, people are more likely to take it seriously"
/>


### Four Eyes Principle (with real relationships)


<boxEmbed
  style="greybox"
  body={<>
    Random Person: _"Hey, can you review this code I wrote with Copilot?"_

Bob: _"I don't know you well, and I have better things to do... approved âœ…"_
  </>}
  figurePrefix="bad"
  figure="Bad example - Random pair reviews, people don't know each other well, so they won't feel accountable"
/>



<boxEmbed
  style="greybox"
  body={<>
    Alice: _"Hey Bob, can you review this code I wrote with Copilot?"_

Bob: _"Sure, I want to make sure I don't let Alice down, so I'll take a closer look ðŸ‘€"_
  </>}
  figurePrefix="good"
  figure="Good example - Pair reviews with people who know each other well, so they feel accountable"
/>


### Information, not recommendation

Ask AI to present information and sources before it asserts conclusions. Avoid instructive "do this" prompts.


<boxEmbed
  style="greybox"
  body={<>
    _"Write a summary of diabetes treatments"_
  </>}
  figurePrefix="bad"
  figure="Bad example - AI provides a summary, with not much context. People may take it at face value"
/>



<boxEmbed
  style="greybox"
  body={<>
    "I am writing a university essay on treatments for diabetes. The essay question is {xyz}, I have these sources {abc}, my thoughts are {123}, and I want to make sure I cover {these points}. Can you help me draft an outline?"_
  </>}
  figurePrefix="good"
  figure="Good example - AI has context, and is less likely to hallucinate. You can more easily take ownership of the work."
/>


#### Four directing questions (readiness gate before prompting)

Use these to decide if you know enough to prompt. If any answer is vague, pause and gather info first.

* **Who is this for?**
* **What problem are we solving right now?**
* **What does success look like ([acceptance criteria](/acceptance-criteria/))?**
* **What information/context do you already have, and whatâ€™s missing?**

---

## Why this matters now

Big Tech is heavily pushing **agents**. We love agents, however, without proper guardrails, we'll let text generators quietly make business decisions.

Recognise automation bias, design for it, and keep people accountable for their work.