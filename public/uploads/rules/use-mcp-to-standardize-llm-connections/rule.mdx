---
type: rule
title: Do you use MCP to standardize connections between your LLMs/Agents and external services?
seoDescription: Learn why using the Model Context Protocol (MCP) is a game-changer for connecting LLMs and agents to external services in a standardized, scalable way.
uri: use-mcp-to-standardize-llm-connections
related:
 - mcp-server
 - use-ai-manage-backlog
authors:
  - title: Hajir Lesani
    url: https://www.ssw.com.au/people/hajir-lesani/
  - title: Louis Roa
    url: https://www.ssw.com.au/people/louis-roa/
  - title: Anton Polkanov
    url: https://www.ssw.com.au/people/anton-polkanov/
created: 2025-04-23T05:55:49.000Z
guid: 4426ed73-3e70-44c3-befd-0b4170d93205
lastUpdated: 2025-09-16T05:25:27.000Z
lastUpdatedBy: Eli Kent [SSW]
lastUpdatedByEmail: 69125238+kulesy@users.noreply.github.com
createdBy: Louis Roa [SSW]
createdByEmail: 149936952+louisroa8189@users.noreply.github.com
isArchived: false
---

<introEmbed
  body={<>
Connecting an LLM-driven agent to multiple external services might look simple in a diagram, but it's often a nightmare in practice.

Each service requires a custom integration, from decoding API docs, handling auth, setting permissions, to mapping strange data formats. And when you build it all directly into your agent or app, it becomes a brittle, tangled mess that's impossible to reuse.
  </>}
/>

<imageEmbed
  alt="Image"
  size="large"
  showBorder={false}
  figureEmbed={ {
    preset: "default",
    figure: "MCP Architecture",
    shouldDisplay: true
  } }
  src="/uploads/rules/use-mcp-to-standardize-llm-connections/mcp.png"
/>


**Image credit:** [Norah Sakal](https://www.linkedin.com/in/norah-klintberg-sakal/)

## What is MCP?

MCP (Model Context Protocol) is a standardized way for LLMs and agents to interact with external APIs and tools‚Äîthink of it as **USB-C for AI**. Instead of manually wiring integrations into each agent or app, MCP centralizes them on a server. Agents speak a single protocol, and the server handles:

* Authentication and authorization (OAuth 2.1 supported)
* Data formatting and transformation
* Service-specific quirks and rate limits

This architecture separates logic and plumbing from your agent, making development faster, integrations cleaner, and reusability a breeze.

### MCP architecture breakdown

The main components:

**1. Host** - An application that wants to use backend services. E.g. an GitHub Copilot or Claude Desktop

**2. MCP Client** - A thin layer on the host. It implements MCP protocol and maintans 1-to-1 connection with the corresponding MCP Server

**3. MCP Server** - The smart middleman that connects to APIs and handles logic

**4. Backend API** - The actual external service like Slack, GitHub, or your internal CRM


<asideEmbed
  variant="greybox"
  body={<>
    **Flow:** Host ‚á® MCP Client ‚á® MCP Server ‚á® External API
  </>}
  figureEmbed={{
    preset: "default",
    figure: "XXX",
    shouldDisplay: false
  }}
/>


### ‚úÖ Benefits of using MCP

* **Write once, use anywhere** - Build an integration once in the MCP server and point any MCP-compatible app at it
* **Keep agents lightweight** - Agent logic stays clean and generic
* **Dynamic discovery** -  Models can see available tools and prompts at runtime, without needing hardcoded instructions
* **Improved reliability** - Core logic lives in server code, not fragile prompt text
* **Chaining tools** - Create powerful workflows by chaining steps (e.g. summarize a spreadsheet, post result to Slack) as a single tool


<asideEmbed
  variant="greybox"
  body={<>
    Your LLM needs to talk to Slack, GitHub, and your internal support ticketing system. Without MCP, you'd build 3 custom plugins. With MCP, you build 3 connectors into the MCP server, and **every agent instantly gets access**.
  </>}
  figureEmbed={{
    preset: "default",
    figure: "XXX",
    shouldDisplay: false
  }}
/>


### Tools, prompts, resources ‚Äì all in one protocol

The **server** can expose:

* **Tools** - Callables like `sendSlackMessage`, `createGitHubIssue`
* **Prompts** - Custom behavior templates, like a "friendly escalation email"
* **Resources** - Contextual data models, documents, or knowledge graphs

The **agent** can offer:

* **Roots** - File access when needed
* **Sampling** - Let the server trigger generation tasks

All communication is standardized and discoverable.

### Who's already using MCP?

* Anthropic invented it
* **Google, Microsoft, and OpenAI** are backing it
* **Zapier** wrapped 7,000+ SaaS integrations as an MCP gateway
* IDEs like **VS Code, Cursor, and JetBrains** are MCP-native
* **Docker, Postman, GitHub** are building servers

### USB-C for AI: Not just a buzzword

Like USB-C unified device ports, MCP is set to unify LLM integrations. The protocol evolves openly, adoption is growing, and contributors span the AI ecosystem.

Betting against MCP is like betting against standards that make life easier.

---

## How to get started

Whether you're building the agent side or the server:

* üõ†Ô∏è **[Server Quick Start](https://modelcontextprotocol.io/quickstart/server)** - Perfect for devs making reusable tools
* üß† **[Client Quick Start](https://modelcontextprotocol.io/quickstart/client)** - If you're building apps that want to call those tools
* üìö Browse [example servers](https://modelcontextprotocol.io/examples) and [client list](https://modelcontextprotocol.io/clients) to see the growing ecosystem

### Try it yourself

Want to get hands-on with MCP? Here's how to get started with one of the useful MCPs called **Context7**.

LLMs rely on outdated or generic information about the libraries you use. Context7 pulls **up-to-date, version-specific documentation and code examples** directly from the source.

While using AI code editors like Cursor or VSCode, **include the phrase "use context7"** in your prompt, and it'll automatically retrieve relevant information from official sources. This helps AI assistant generate accurate and current code snippets.

* **Step 1** - Go to [context7 repo](https://github.com/upstash/context7) and read the setup guide (Cursor, VSCode, Claude Desktop, etc)

* **Step 2** - Once installed, try asking a question using context7\
  Example: In your Chat panel, type: "Add tailwind to my project. Use context7"

  ‚ú® **Note:** Make sure you're in **Agent Mode**

   Cursor/VSCode will auto‚Äëdiscover two tools exposed by Context7: resolve-library-id ‚ûú get-library-docs. It resolves "tailwind" to the latest compatible ID and injects concise, version‚Äëaccurate code snippets into the conversation

* **Step 3** - Verify that tailwind (version 4) is added

* **Step 4** - Try these steps without using context7 and compare your results

#### What you just proved?

By using context7, you are referring to the **latest docs**, and it will **reduce bugs and errors** in your code.

Context7 is just one of many MCP servers you can use to improve your experience with LLMs. There are other great tools such as **GitHub MCP**, **Browser Use MCP** (page navigation, form filling, reading console messages), **Sequential Thinking**, and **Playwright MCP** (helps with automate testing and browser interactions).

More servers can be found on [MCP Server Directory](https://www.pulsemcp.com/servers).

---

## Need help connecting your proprietary services?

üì© [Reach out to SSW](https://www.ssw.com.au/consulting/artificial-intelligence) ‚Äì we'll help you build a robust, scalable MCP server.
