---
type: rule
title: What is GPT?
uri: what-is-gpt
authors:
  - title: Calum Simpson
    url: https://www.ssw.com.au/people/calum-simpson
  - title: Piers Sinclair
    url: https://www.ssw.com.au/people/piers-sinclair
  - title: Adam Cogan
    url: https://www.ssw.com.au/people/adam-cogan
  - title: Harry Ross
    url: https://www.ssw.com.au/people/harry-ross
  - title: Jack Reimers
    url: https://www.ssw.com.au/people/jack-reimers
created: 2023-02-08T05:16:30.682Z
guid: 2d5202d2-32a9-494e-b8ce-c507935afbfe
redirects:
  - what-is-gpt-3
---
GPT is a large language model developed by [OpenAI](https://openai.com) with major financial backing from Microsoft, and is currently leading the market in terms of technology and capability.

<!--endintro-->

AI technologies have been being developed at companies like Amazon, Google and Microsoft for some time. The first large language model, GPT-1, was released by OpenAI in 2018.  
It wasn't until the release of ChatGPT, which is powered by GPT, that the technology became widely known.

### Isn't it called ChatGPT?
Whilst sharing similar names ChatGPT and GPT are different.  
GPT stands for Generative Pretrained Transformer and refers to the large language model that underlies the ChatGPT frontend. This model accessed using an API.  

This allows for developers to build their own applications leveraging the same capabilities of ChatGPT over the top of the GPT model.
A great example of this is the [SSW RulesGPT bot](https://rulesgpt.ssw.com.au/).

### Why is GPT cool?
::: greybox
Why is GPT cool?

GPT is cool because it is a powerful artificial intelligence language model that can generate human-like text. It can be used for a variety of tasks, such as natural language processing, text generation, and question answering. It has the potential to revolutionize the way people interact with computers and make AI more accessible to the general public.
:::
**Figure: ChatGPT's response when asked "Why is GPT cool?"**

### How does GPT work?
GPT models are trained on a huge amount of data (often refered to as parameters) that it can then use to predict what words should come next when given an input.  
For example, if the training data contains a weather report talking about beautiful clear blue skies, when you ask it what color the sky is it can call back to that data and respond blue.  

GPT is trained on a huge amount of parameters.
* GPT-4: 1.76 **trillion** parameters.
* GPT-3: 175 billion parameters.
* GPT-2: 1.5 billion parameters.
* GPT-1: 117 million parameters.

### Strengths
* The model can respond to queries using natural language.
* The model can handle grammatical and spelling errors.
* Responses can come from the model very quickly - no need to access a database to retrieve data.

### Limitations
* The model is only as good as it's training data and has no knowledge of anything outside of this data.

Try it out in the [playground](https://platform.openai.com/playground)
